<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="initial-scale=1, width=device-width" />

    <link rel="stylesheet" href="./global.css" />
    <link rel="stylesheet" href="./index.css" />
    <link
      rel="stylesheet"
      href="https://fonts.googleapis.com/css2?family=Work Sans:wght@400;600;700&display=swap"
    />
    <link
      rel="stylesheet"
      href="https://fonts.googleapis.com/css2?family=Inter:wght@600;900&display=swap"
    />
  </head>
  <body>
      <div class="home-page">
          <div class="top">
              <div class="cs1102-project-topic">
                  CS1102 PROJECT TOPIC - THE DEVELOPMENT OF NEURAL NETWORKS
              </div>
              <div class="Neural-networks">NEURAL NETWORKS </div>
              <div class="group-no-25-parent">
                  <div class="group-no-25">Group No. 25:</div>
                  <div class="student-name">Student Name:</div>
                  <div class="student-id">Student ID:</div>
                  <div class="jack-cooper">Jack Cooper</div>
                  <div class="div">40152099</div>
                  <div class="david-lee">David Lee</div>
                  <div class="div1">55752993</div>
                  <div class="kim-kangmin">Kim Kangmin</div>
                  <div class="div2">56314303</div>
                  <div class="sean-chiang-shang-en">Sean Chiang Shang-en</div>
                  <div class="div3">57159310</div>
              </div>
          </div>
          <div class="home-page-child"></div>
          <img class="cicles-icon" alt="" src="./public/cicles.svg" />
          <button class="button">
              <div class="button1">Home</div>
          </button>
          <button class="button2" id="button1">
              <div class="button1">Reflections</div>
          </button>
          <button class="button4" id="button3">
              <div class="button5">Try It Out</div>
          </button>
         <div class="body">
        <b class="introduction">Introduction</b>
        <b class="history-of-mlp">History of MLP</b>
        <b class="comparison-with-cnn">Comparison with CNN and RNN:</b>
        <b class="challenges-and-criticisms-container">
          <span class="challenges-and-criticisms-container1">
            <p class="challenges-and-criticisms">
              Challenges and Criticisms of MLP:
            </p>
          </span>
        </b>
        <b class="addressing-mlps-limitations-container">
          <span class="challenges-and-criticisms-container1">
            <p class="challenges-and-criticisms">
              Addressing MLP's Limitations:
            </p>
          </span>
        </b>
        <div class="multilayer-perceptron-mlp-container">
          <span class="challenges-and-criticisms-container1">
            <p class="challenges-and-criticisms">
              Multilayer Perceptron (MLP) is a fundamental neural network
              architecture widely used in deep learning. It consists of
              interconnected artificial neurons, allowing it to approximate any
              function under certain conditions. MLP has played a crucial role
              in the development of deep learning and neural network research.
            </p>
          </span>
        </div>
        <div class="the-concept-of-container">
          <span class="challenges-and-criticisms-container1">
            <p class="challenges-and-criticisms">
              The concept of artificial neural networks dates back to the 1940s,
              but significant advancements in training MLP models using the
              backpropagation algorithm occurred in the 1980s and 1990s. These
              developments led to the wide adoption of MLP in various domains,
              including pattern recognition, speech recognition, and data
              analysis.
            </p>
          </span>
        </div>
        <div class="multilayer-perceptrons-mlp-container">
          <span class="challenges-and-criticisms-container1">
            <p class="challenges-and-criticisms">
              Multilayer Perceptrons (MLP), Convolutional Neural Networks (CNN),
              and Recurrent Neural Networks (RNN) are all popular neural network
              architectures, each with its unique characteristics. According to
              Iqbal (2021), 
            </p>
            <ul class="mlp-is-a-fully-connected-netwo">
              <li class="mlp-is-a">
                MLP is a fully connected network suitable for simple image
                classification tasks.
              </li>
              <li class="mlp-is-a">
                CNN is good at processing grid-like data, such as images.
              </li>
              <li>
                RNN handles sequential data well by incorporating feedback
                connections.
              </li>
            </ul>
          </span>
        </div>
        <div class="despite-its-success-container">
          <span class="challenges-and-criticisms-container1">
            <p class="challenges-and-criticisms">
              Despite its success, MLP has certain limitations:
            </p>
            <ul class="mlp-is-a-fully-connected-netwo">
              <li class="mlp-is-a">
                Interpretability: MLP models are often considered black boxes,
                making it challenging to understand how they arrive at their
                predictions (Iqbal 2021).
              </li>
              <li class="mlp-is-a">
                Vulnerability to adversarial attacks: MLPs can be deceived by
                carefully crafted input perturbations, leading to incorrect
                predictions (Afnan and Murad 2023). 
              </li>
              <li class="mlp-is-a">
                Bias and fairness issues: MLP models can inherit biases present
                in the training data, potentially leading to discriminatory
                outcomes (Gregor, et al. 2023).
              </li>
              <li class="mlp-is-a">
                Scalability and generalization challenges: As MLPs grow in size
                and complexity, they may struggle with scalability and
                generalization to unseen data(Gregor, et al. 2023). 
              </li>
              <li>
                Potential for misuse and manipulation: MLP models can be
                intentionally manipulated or exploited for malicious purposes
                (Wai, et al. 2023).
              </li>
            </ul>
          </span>
        </div>
        <div class="researchers-and-practitioners-container">
          <span class="challenges-and-criticisms-container1">
            <p class="challenges-and-criticisms">
              Researchers and practitioners are actively working on mitigating
              the challenges associated with MLP:
            </p>
            <ul class="mlp-is-a-fully-connected-netwo">
              <li class="mlp-is-a">
                Improving interpretability: Techniques like attention
                mechanisms, layer-wise relevance propagation, and model
                distillation are being explored to enhance the interpretability
                of MLP models (Saptarshi, et al. 2019).
              </li>
              <li class="mlp-is-a">
                Defending against adversarial attacks: Adversarial training,
                input regularization, and defensive distillation are some
                techniques used to increase the robustness of MLP models against
                adversarial attacks (Clavance 2019).
              </li>
              <li class="mlp-is-a">
                Incorporating fairness constraints: Researchers are
                investigating ways to incorporate fairness metrics and
                constraints during the training process to reduce bias and
                ensure equitable outcomes (Parushi, et al. 2021).
              </li>
              <li class="mlp-is-a">
                Advancing neural network architectures: Novel architectures,
                such as capsule networks, graph neural networks, and
                transformers, are being developed to overcome the limitations of
                MLP and improve performance on specific tasks (Kofi and Richard
                2023).
              </li>
              <li>
                Implementing safeguards: Efforts are being made to establish
                ethical guidelines, transparency measures, and regulatory
                frameworks to prevent the misuse and manipulation of MLP models
                (Mridha, et al. 2021).
              </li>
            </ul>
          </span>
        </div>
      </div>

    <script>
      var button1 = document.getElementById("button1");
      if (button1) {
        button1.addEventListener("click", function (e) {
          window.location.href = "./reflections.html";
        });
      }
      
      var button3 = document.getElementById("button3");
      if (button3) {
        button3.addEventListener("click", function (e) {
          window.location.href = "http://172.28.190.193:8502/";
        });
      }
      </script>
  </body>
</html>
